{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0749ddf-48e2-48fa-8221-b15817053e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchsummary import summary\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch.nn as nn\n",
    "from torch.nn import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835426fb-5df6-411d-abe6-226afa19c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e2d936-3f88-4234-8545-7b15aacebdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.inception_v3(pretrained=True).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33923ea6-f13d-4a0c-b406-b7300878e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_dummy(image):\n",
    "    mask = image[:, :, -1]\n",
    "    img = image[:, :, :3]\n",
    "    img = cv2.bitwise_and(img, img, mask=mask)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49db78a-09cf-4d1f-9595-a14c3bdcf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txtfile, root, transform=None):\n",
    "        self.names = open(txtfile, 'r').read().splitlines()\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root, self.names[index])\n",
    "        img = cv2.imread(img_path, -1)\n",
    "        img = rem_dummy(img)\n",
    "        img = cv2.resize(img, (224, 224), interpolation = cv2.INTER_AREA)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfb2ab4-b55d-4681-90d0-4922058f5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                                         (0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "batch_size = 64\n",
    "txtfile = r\"C:\\Users\\Area 51\\Desktop\\Project\\data\\img_names.txt\"\n",
    "rootdir = r\"C:\\Users\\Area 51\\Desktop\\Project\\data\\bottoms_resized_png\"\n",
    "\n",
    "dataset = MyDataset(txtfile, rootdir, transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dffe85b3-1701-4eab-9a81-4df79c572234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_matrix(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    demo_tnsr = torch.randn((1, 3, 224, 224)).to('cuda')\n",
    "    demo = model(demo_tnsr)\n",
    "    emb_mat = torch.randn_like(demo).cpu()\n",
    "    with torch.no_grad():\n",
    "        for images in dataloader:\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            feat_mat = model(images).cpu()\n",
    "            \n",
    "            emb_mat = torch.cat((emb_mat, feat_mat), 0)\n",
    "    torch.save(emb_mat, r\"C:\\Users\\Area 51\\Desktop\\embeddings.pt\")        \n",
    "    return emb_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0d52f2-d59f-4e9c-9b7a-b206bc5a8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_images(image, num_images, emb_mat):\n",
    "    image = rem_dummy(image)\n",
    "    image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feat_mat = model(image).cpu()\n",
    "    feat_mat = feat_mat.reshape((feat_mat.shape[0], -1))\n",
    "    emb_mat = emb_mat.reshape((emb_mat.shape[0], -1))\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=num_images, metric='cosine')\n",
    "    knn.fit(emb_mat)\n",
    "    \n",
    "    _, ind = knn.kneighbors(feat_mat)\n",
    "    ind_lst = ind.tolist()[0][1:]\n",
    "\n",
    "    return ind_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b897d69f-9b3d-490c-8163-759b6429f554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "emb_mat = feature_matrix(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "993be87d-aef0-4300-ba21-d384d4e703b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r\"C:\\Users\\Area 51\\Desktop\\Project\\data\\bottoms_resized_png\\13583498OWD.png\", -1)\n",
    "ind_lst = similar_images(image, 11, emb_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67158f71-98ad-4335-b38f-7627d109eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(txtfile, 'r').read().splitlines()\n",
    "import numpy as np\n",
    "c = 1\n",
    "input_img = cv2.imread(r\"C:\\Users\\Area 51\\Desktop\\Project\\data\\bottoms_resized_png\\13583498OWD.png\", -1)\n",
    "inp = cv2.resize(input_img, None, fx=2, fy=2)\n",
    "inp = rem_dummy(inp)\n",
    "p = []\n",
    "for i in ind_lst:\n",
    "    img = cv2.imread(os.path.join(rootdir, names[i-1]), -1)\n",
    "    \n",
    "    img = rem_dummy(img)\n",
    "    p.append(img)\n",
    "\n",
    "a = np.concatenate((p[0], p[1], p[2], p[3], p[4]), axis=1)\n",
    "b = np.concatenate((p[5], p[6], p[7], p[8], p[9]), axis=1)\n",
    "c = np.concatenate((a, b), axis=0)\n",
    "d = np.concatenate((inp, c), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d01ee7b2-c39a-42a1-a30c-fb07f1e78f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(r\"C:\\Users\\Area 51\\Desktop\\result.jpg\", d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
